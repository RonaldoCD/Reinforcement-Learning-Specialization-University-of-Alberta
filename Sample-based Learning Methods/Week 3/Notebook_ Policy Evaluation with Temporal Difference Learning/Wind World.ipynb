{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "favorite-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "# --\n",
    "import numpy as np\n",
    "# --\n",
    "from rl_glue import RLGlue\n",
    "# --\n",
    "from Agent import BaseAgent \n",
    "from Environment import BaseEnvironment  \n",
    "# --\n",
    "from manager import Manager\n",
    "# --\n",
    "from itertools import product\n",
    "# --\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "included-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty CliffWalkEnvironment class.\n",
    "# These methods will be filled in later cells.\n",
    "class WindyWorld(BaseEnvironment):\n",
    "    def env_init(self, env_info={}):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_start(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_end(self, reward):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def env_cleanup(self, reward):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # helper method\n",
    "    def state(self, loc):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "manual-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyWorld\n",
    "def env_init(self, env_info={}):\n",
    "    \"\"\"Setup for the environment called when the experiment first starts.\n",
    "    Note:\n",
    "    Initialize a tuple with the reward, first state, boolean\n",
    "    indicating if it's terminal.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Note, we can setup the following variables later, in env_start() as it is equivalent. \n",
    "    # Code is left here to adhere to the note above, but these variables are initialized once more\n",
    "    # in env_start() [See the env_start() function below.]\n",
    "        \n",
    "    reward = None\n",
    "    state = None # See Aside\n",
    "    termination = None\n",
    "    self.reward_state_term = (reward, state, termination)\n",
    "        \n",
    "    # Assign width and height of the enviroment\n",
    "    self.grid_w = env_info.get('grid_width', 10)\n",
    "    self.grid_h = env_info.get('grid_height', 7)\n",
    "        \n",
    "    # Assign goal state and start state for the agent\n",
    "    self.start_loc = (self.grid_h//2, 0)\n",
    "    self.goal_loc = (self.grid_h//2, self.grid_w - 3)\n",
    "        \n",
    "    self.winds = env_info.get('winds', [0, 0, 0, 1, 1, 1, 2, 2, 1, 0])\n",
    "        \n",
    "    assert (len(self.winds) == self.grid_w)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "signal-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyWorld\n",
    "def state(self, loc):\n",
    "    # Return the state for a location. Each loc has its code\n",
    "    loc_cod = loc[0] * self.grid_w + loc[1]\n",
    "    return loc_cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "italic-terminal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3, 4, 5]), array([1, 2, 3, 4, 5]), 'a', 'a']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.append('a')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "important-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyWorld\n",
    "\n",
    "def display_policy(self, policy):\n",
    "    policy_graph = np.zeros((self.grid_h, self.grid_w))\n",
    "    for i in range(self.grid_h):\n",
    "        for j in range(self.grid_w):\n",
    "            loc = (i, j)\n",
    "            loc_cod = self.state(loc) \n",
    "            best_action = np.argmax(policy[loc_cod])\n",
    "            if best_action == 0:\n",
    "                policy_graph[i,j] = 0\n",
    "            elif best_action == 1:\n",
    "                policy_graph[i,j] = 1\n",
    "            elif best_action == 2:\n",
    "                policy_graph[i,j] = 2\n",
    "            else:\n",
    "                policy_graph[i,j] = 3\n",
    "    print('------------------------------------------')\n",
    "    print(policy_graph)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "solar-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyWorld\n",
    "def env_start(self):\n",
    "    \"\"\"The first method called when the episode starts, called before the\n",
    "    agent starts.\n",
    "\n",
    "    Returns:\n",
    "        The first state from the environment.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    self.agent_loc = self.start_loc\n",
    "    state = self.state(self.agent_loc)\n",
    "    termination = False\n",
    "    self.reward_state_term = (reward, state, termination)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "recovered-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyWorld\n",
    "\n",
    "def env_step(self, action):\n",
    "    \"\"\"A step taken by the environment.\n",
    "\n",
    "    Args:\n",
    "        action: The action taken by the agent\n",
    "\n",
    "    Returns:\n",
    "        (float, state, Boolean): a tuple of the reward, state,\n",
    "            and boolean indicating if it's terminal.\n",
    "    \"\"\"\n",
    "#     print('--------------')\n",
    "#     print('Action: ',action)\n",
    "#     print('Initial Agent loc: ',self.agent_loc)\n",
    "    state_wind = self.winds[self.agent_loc[1]]\n",
    "    loc_after_wind = None\n",
    "    if self.agent_loc[0] - state_wind >= 0:\n",
    "        loc_after_wind = (self.agent_loc[0] - state_wind, self.agent_loc[1])\n",
    "    elif self.agent_loc[0] - state_wind == -1:\n",
    "        loc_after_wind = (0, self.agent_loc[1])\n",
    "    else:\n",
    "        loc_after_wind = (self.agent_loc[0], self.agent_loc[1])\n",
    "    \n",
    "#     print('After wind loc: ',loc_after_wind, ' Wind: ', state_wind)\n",
    "    \n",
    "    if action == 0: # UP (Task 1)\n",
    "        ### START CODE HERE ###\n",
    "        # Hint: Look at the code given for the other actions and think about the logic in them.\n",
    "        # Stay.\n",
    "        possible_next_loc = (loc_after_wind[0] - 1, loc_after_wind[1])\n",
    "        if possible_next_loc[0] >= 0:\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass\n",
    "        ### END CODE HERE ###\n",
    "    elif action == 1: # LEFT\n",
    "        possible_next_loc = (loc_after_wind[0], loc_after_wind[1] - 1)\n",
    "        if possible_next_loc[1] >= 0: # Within Bounds?\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass # Stay.\n",
    "    elif action == 2: # DOWN\n",
    "        possible_next_loc = (loc_after_wind[0] + 1, loc_after_wind[1])\n",
    "        if possible_next_loc[0] < self.grid_h: # Within Bounds?\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass # Stay.\n",
    "    elif action == 3: # RIGHT\n",
    "        possible_next_loc = (loc_after_wind[0], loc_after_wind[1] + 1)\n",
    "        if possible_next_loc[1] < self.grid_w: # Within Bounds?\n",
    "            self.agent_loc = possible_next_loc\n",
    "        else:\n",
    "            pass # Stay.\n",
    "    else: \n",
    "        raise Exception(str(action) + \" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!\")\n",
    "    \n",
    "#     print('Final Agent loc: ',self.agent_loc)\n",
    "    reward = -1\n",
    "    terminal = False\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Hint: Consider the initialization of reward and terminal variables above. Then, note the \n",
    "    # conditional statements and comments given below and carefully ensure to set the variables reward \n",
    "    # and terminal correctly for each case.\n",
    "    if self.agent_loc == self.goal_loc: # Reached Goal!\n",
    "        terminal = True      \n",
    "    ### END CODE HERE ###\n",
    "#     print('New Location ',self.agent_loc)\n",
    "#     print('R_S_T: ', reward, self.state(self.agent_loc), terminal)\n",
    "    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n",
    "    return self.reward_state_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "integrated-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyWorld\n",
    "\n",
    "def env_cleanup(self):\n",
    "    \"\"\"Cleanup done after the environment ends\"\"\"\n",
    "    self.agent_loc = self.start_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "considered-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WWAgent(BaseAgent):\n",
    "    def agent_init(self, agent_info={}):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def agent_cleanup(self):        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update_policy(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bizarre-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WWAgent\n",
    "\n",
    "def agent_init(self, agent_info={}):\n",
    "    \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "\n",
    "    self.grid_h = agent_info.get(\"grid_height\", 7)\n",
    "    self.grid_w = agent_info.get(\"grid_height\", 10)\n",
    "    \n",
    "    \n",
    "    # Create a random number generator with the provided seed to seed the agent for reproducibility.\n",
    "    self.rand_generator = np.random.RandomState(agent_info.get(\"seed\",0))\n",
    "    self.possible_actions = 4\n",
    "    \n",
    "    # Define epsilon\n",
    "    self.epsilon = agent_info.get(\"epsilon\",0.1)\n",
    "    \n",
    "    # Create a random policy based in epsilon - greedy\n",
    "    self.policy = agent_info.get(\"policy\", np.ones((self.grid_w * self.grid_h, self.possible_actions)))\n",
    "    self.policy *= (self.epsilon / (self.possible_actions - 1))\n",
    "\n",
    "    mask = np.random.randint(0, 4, self.policy.shape[0])\n",
    "    self.policy[range(self.policy.shape[0]), mask] = 1 - self.epsilon\n",
    "    \n",
    "    # Discount factor (gamma) to use in the updates.\n",
    "    self.discount = agent_info.get(\"discount\")\n",
    "    # The learning rate or step size parameter (alpha) to use in updates.\n",
    "    self.step_size = agent_info.get(\"step_size\")\n",
    "\n",
    "    # Initialize an array of zeros that will hold the values.\n",
    "    # Recall that the policy can be represented as a (# States, # Actions) array. With the \n",
    "    # assumption that this is the case, we can use the first dimension of the policy to\n",
    "    # initialize the array for values.\n",
    "    self.q_values = np.zeros_like(self.policy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "great-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WWAgent\n",
    "def agent_start(self, state):\n",
    "    \"\"\"The first method called when the episode starts, called after\n",
    "    the environment starts.\n",
    "    Args:\n",
    "        state (Numpy array): the state from the environment's env_start function.\n",
    "    Returns:\n",
    "        The first action the agent takes.\n",
    "    \"\"\"\n",
    "    # The policy can be represented as a (# States, # Actions) array. So, we can use \n",
    "    # the second dimension here when choosing an action.\n",
    "    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
    "    self.last_state = state\n",
    "    self.last_action = action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "distant-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WWAgent\n",
    "def update_policy(self):\n",
    "    max_action = np.argmax(self.q_values[self.last_state])\n",
    "    self.policy[self.last_state] = self.epsilon / (self.possible_actions - 1)\n",
    "    self.policy[self.last_state, max_action] = 1 - self.epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "terminal-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WWAgent\n",
    "def agent_step(self, reward, state, action):\n",
    "    \"\"\"A step taken by the agent.\n",
    "    Args:\n",
    "        reward (float): the reward received for taking the last action taken\n",
    "        state (Numpy array): the state from the\n",
    "        environment's step after the last action, i.e., where the agent ended up after the\n",
    "        last action\n",
    "    Returns:\n",
    "        The action the agent is taking.\n",
    "    \"\"\"\n",
    "    # Update q_values \n",
    "    self.q_values[self.last_state, self.last_action] += self.step_size * (reward + self.discount * self.q_values[state, action] - self.q_values[self.last_state, self.last_action]) \n",
    "    \n",
    "    # Update policy values for last state\n",
    "    self.update_policy()\n",
    "    \n",
    "    # Choose new action\n",
    "    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
    "    self.last_state = state\n",
    "    self.last_action = action\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "valid-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WWAgent\n",
    "def agent_end(self, reward):\n",
    "    \"\"\"Run when the agent terminates.\n",
    "    Args:\n",
    "        reward (float): the reward the agent received for entering the terminal state.\n",
    "    \"\"\"\n",
    "    # Update Q values\n",
    "    self.q_values[self.last_state, self.last_action] += self.step_size * (reward - self.q_values[self.last_state, self.last_action])\n",
    "    \n",
    "    # Update policy values for last state\n",
    "    self.update_policy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "annual-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WWAgent\n",
    "def agent_cleanup(self):\n",
    "    \"\"\"Cleanup done after the agent ends.\"\"\"\n",
    "    self.last_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "jewish-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WWAgent\n",
    "def agent_message(self, message):\n",
    "    \"\"\"A function used to pass information from the agent to the experiment.\n",
    "    Args:\n",
    "        message: The message passed to the agent.\n",
    "    Returns:\n",
    "        The response (or answer) to the message.\n",
    "    \"\"\"\n",
    "    if message == \"get_values\":\n",
    "        return self.q_values\n",
    "    else:\n",
    "        raise Exception(\"TDAgent.agent_message(): Message not understood!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "documented-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env_info, agent_info, \n",
    "                   num_episodes=5000,\n",
    "                   experiment_name=None,\n",
    "                   plot_freq=200,\n",
    "                   true_values_file=None,\n",
    "                   value_error_threshold=1e-8):\n",
    "    env = WindyWorld\n",
    "    agent = WWAgent\n",
    "    rl_glue = RLGlue(env, agent)\n",
    "    rl_glue.rl_init(agent_info, env_info)\n",
    "    policy = rl_glue.agent.policy\n",
    "#     manager = Manager(env_info, policy, true_values_file=true_values_file, experiment_name=experiment_name)\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        rl_glue.rl_episode(0) # no step limit\n",
    "#         print('Episode: ', episode)\n",
    "        if episode % plot_freq == 0:\n",
    "            print('Episode number: ', episode)\n",
    "            q_values = rl_glue.agent.agent_message(\"get_values\")\n",
    "            rl_glue.environment.display_policy(rl_glue.agent.policy)\n",
    "#             print('Q_Values: ', q_values)\n",
    "#             manager.visualize(q_values, episode)\n",
    "\n",
    "    q_values = rl_glue.agent.agent_message(\"get_values\")\n",
    "    #if true_values_file is not None:\n",
    "        # Grading: The Manager will check that the values computed using your TD agent match \n",
    "        # the true values (within some small allowance) across the states. In addition, it also\n",
    "        # checks whether the root mean squared value error is close to 0.\n",
    "        #manager.run_tests(values, value_error_threshold)\n",
    "    \n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "premium-broadway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  500\n",
      "------------------------------------------\n",
      "[[0. 2. 2. 0. 0. 1. 0. 3. 2. 2.]\n",
      " [0. 2. 1. 1. 0. 0. 3. 2. 3. 2.]\n",
      " [2. 0. 3. 2. 1. 2. 0. 0. 2. 2.]\n",
      " [2. 3. 3. 3. 2. 2. 2. 3. 2. 2.]\n",
      " [1. 3. 0. 2. 2. 2. 1. 3. 1. 1.]\n",
      " [1. 1. 3. 3. 2. 0. 0. 3. 2. 2.]\n",
      " [3. 0. 3. 1. 3. 1. 2. 0. 2. 3.]]\n",
      "Episode number:  1000\n",
      "------------------------------------------\n",
      "[[0. 0. 1. 2. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 0. 2. 0. 3. 0. 1. 0. 3. 2.]\n",
      " [3. 1. 1. 1. 2. 2. 0. 0. 2. 2.]\n",
      " [3. 0. 2. 0. 2. 2. 0. 3. 2. 2.]\n",
      " [2. 1. 1. 2. 2. 0. 1. 2. 1. 1.]\n",
      " [3. 1. 3. 2. 2. 0. 0. 2. 2. 3.]\n",
      " [0. 2. 3. 3. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  1500\n",
      "------------------------------------------\n",
      "[[0. 2. 1. 2. 1. 3. 3. 2. 3. 2.]\n",
      " [1. 1. 2. 0. 0. 3. 1. 3. 0. 2.]\n",
      " [3. 3. 3. 2. 2. 2. 3. 1. 1. 2.]\n",
      " [0. 0. 2. 2. 1. 2. 0. 3. 1. 2.]\n",
      " [2. 3. 3. 2. 2. 2. 1. 2. 1. 1.]\n",
      " [3. 1. 3. 2. 3. 0. 0. 3. 1. 1.]\n",
      " [1. 1. 0. 2. 3. 1. 2. 0. 3. 0.]]\n",
      "Episode number:  2000\n",
      "------------------------------------------\n",
      "[[3. 3. 2. 2. 3. 3. 2. 3. 3. 2.]\n",
      " [1. 0. 2. 0. 0. 0. 3. 3. 3. 2.]\n",
      " [0. 0. 2. 1. 3. 2. 0. 0. 2. 2.]\n",
      " [1. 0. 2. 2. 2. 3. 0. 3. 2. 2.]\n",
      " [0. 3. 2. 2. 2. 2. 1. 2. 1. 1.]\n",
      " [1. 2. 0. 0. 2. 0. 0. 2. 0. 2.]\n",
      " [3. 3. 2. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  2500\n",
      "------------------------------------------\n",
      "[[0. 2. 0. 2. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 1. 0. 3. 0. 2. 3. 3. 3. 2.]\n",
      " [1. 3. 0. 2. 0. 2. 0. 0. 2. 2.]\n",
      " [3. 3. 3. 0. 2. 0. 1. 3. 3. 2.]\n",
      " [2. 3. 2. 1. 2. 3. 1. 2. 1. 1.]\n",
      " [3. 2. 3. 3. 2. 0. 0. 2. 1. 1.]\n",
      " [1. 3. 1. 1. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  3000\n",
      "------------------------------------------\n",
      "[[3. 0. 0. 2. 3. 3. 3. 2. 3. 2.]\n",
      " [2. 0. 3. 1. 3. 0. 0. 3. 3. 2.]\n",
      " [2. 3. 0. 3. 2. 2. 0. 0. 2. 2.]\n",
      " [3. 1. 0. 3. 0. 1. 1. 3. 1. 2.]\n",
      " [3. 3. 3. 2. 2. 2. 1. 2. 1. 1.]\n",
      " [1. 2. 0. 1. 2. 0. 0. 2. 2. 2.]\n",
      " [3. 0. 3. 3. 3. 1. 2. 0. 3. 2.]]\n",
      "Episode number:  3500\n",
      "------------------------------------------\n",
      "[[0. 1. 1. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 2. 1. 0. 3. 3. 3. 1. 2. 2.]\n",
      " [2. 3. 0. 3. 2. 0. 0. 0. 2. 2.]\n",
      " [2. 3. 2. 3. 2. 0. 3. 3. 2. 2.]\n",
      " [0. 3. 2. 2. 0. 1. 1. 2. 1. 1.]\n",
      " [2. 2. 2. 2. 3. 0. 0. 0. 2. 1.]\n",
      " [3. 0. 0. 2. 3. 1. 2. 0. 3. 2.]]\n",
      "Episode number:  4000\n",
      "------------------------------------------\n",
      "[[0. 0. 2. 2. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 2. 2. 1. 3. 3. 3. 3. 2. 2.]\n",
      " [3. 2. 1. 2. 0. 3. 0. 0. 2. 2.]\n",
      " [0. 2. 2. 2. 2. 0. 3. 3. 2. 2.]\n",
      " [3. 3. 0. 3. 2. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 2. 3. 2. 0. 0. 2. 2. 1.]\n",
      " [3. 3. 0. 2. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  4500\n",
      "------------------------------------------\n",
      "[[1. 0. 1. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 2. 1. 3. 3. 3. 3. 0. 3. 2.]\n",
      " [0. 1. 1. 3. 2. 1. 0. 3. 3. 2.]\n",
      " [1. 2. 3. 2. 3. 1. 3. 3. 3. 2.]\n",
      " [3. 2. 3. 2. 2. 1. 1. 2. 1. 1.]\n",
      " [1. 3. 3. 1. 3. 0. 0. 2. 3. 3.]\n",
      " [1. 2. 3. 3. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  5000\n",
      "------------------------------------------\n",
      "[[0. 0. 2. 2. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 3. 3. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [1. 1. 3. 3. 3. 3. 3. 0. 0. 2.]\n",
      " [2. 2. 2. 1. 2. 3. 3. 3. 0. 2.]\n",
      " [2. 2. 2. 2. 3. 2. 1. 2. 1. 1.]\n",
      " [3. 1. 1. 3. 0. 0. 0. 3. 1. 2.]\n",
      " [1. 0. 3. 3. 3. 1. 2. 0. 1. 0.]]\n",
      "Episode number:  5500\n",
      "------------------------------------------\n",
      "[[3. 0. 1. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 1. 0. 0. 3. 3. 3. 0. 3. 2.]\n",
      " [1. 0. 3. 2. 3. 3. 2. 0. 2. 2.]\n",
      " [2. 0. 0. 3. 3. 3. 3. 3. 2. 2.]\n",
      " [2. 3. 3. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [0. 3. 2. 3. 1. 0. 0. 3. 1. 3.]\n",
      " [1. 2. 3. 2. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  6000\n",
      "------------------------------------------\n",
      "[[2. 3. 1. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 2. 3. 3. 3. 3. 3. 0. 3. 2.]\n",
      " [0. 3. 0. 3. 3. 2. 0. 0. 2. 2.]\n",
      " [2. 3. 0. 3. 2. 2. 2. 3. 2. 2.]\n",
      " [2. 2. 3. 3. 2. 3. 1. 2. 1. 1.]\n",
      " [2. 2. 3. 3. 2. 0. 0. 2. 1. 1.]\n",
      " [1. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  6500\n",
      "------------------------------------------\n",
      "[[2. 2. 3. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 0. 3. 3. 1. 3. 2. 3. 0. 2.]\n",
      " [3. 0. 3. 1. 3. 1. 3. 0. 2. 2.]\n",
      " [3. 3. 1. 3. 0. 0. 0. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 2. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 2. 1. 0. 0. 2. 1. 3.]\n",
      " [1. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  7000\n",
      "------------------------------------------\n",
      "[[2. 2. 3. 0. 3. 2. 3. 3. 3. 2.]\n",
      " [3. 1. 3. 0. 3. 0. 3. 0. 3. 2.]\n",
      " [3. 0. 2. 0. 3. 3. 3. 2. 3. 2.]\n",
      " [2. 3. 3. 3. 3. 0. 2. 3. 3. 2.]\n",
      " [2. 3. 2. 2. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 1. 3. 0. 0. 0. 0. 2. 1. 2.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  7500\n",
      "------------------------------------------\n",
      "[[2. 0. 3. 3. 2. 3. 3. 3. 3. 2.]\n",
      " [1. 3. 2. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 0. 3. 0. 3. 2. 3. 0. 2. 2.]\n",
      " [0. 3. 3. 3. 0. 1. 2. 3. 2. 2.]\n",
      " [3. 2. 3. 2. 3. 3. 1. 2. 1. 1.]\n",
      " [2. 2. 3. 3. 3. 0. 0. 2. 1. 3.]\n",
      " [1. 1. 3. 2. 3. 1. 2. 0. 3. 1.]]\n",
      "Episode number:  8000\n",
      "------------------------------------------\n",
      "[[1. 0. 3. 3. 3. 2. 2. 3. 3. 2.]\n",
      " [0. 3. 0. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [1. 0. 2. 2. 3. 3. 0. 0. 0. 2.]\n",
      " [2. 2. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 3. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 0. 0. 3. 1. 2.]\n",
      " [0. 2. 3. 3. 3. 1. 2. 0. 3. 1.]]\n",
      "Episode number:  8500\n",
      "------------------------------------------\n",
      "[[0. 2. 3. 3. 0. 3. 3. 3. 3. 2.]\n",
      " [3. 3. 0. 3. 3. 3. 3. 1. 3. 2.]\n",
      " [0. 3. 3. 0. 3. 3. 0. 0. 2. 2.]\n",
      " [2. 2. 3. 3. 0. 3. 2. 3. 2. 2.]\n",
      " [3. 2. 3. 2. 3. 0. 1. 2. 1. 1.]\n",
      " [3. 3. 2. 3. 3. 0. 0. 2. 2. 1.]\n",
      " [1. 3. 1. 3. 3. 1. 2. 0. 2. 1.]]\n",
      "Episode number:  9000\n",
      "------------------------------------------\n",
      "[[0. 3. 2. 1. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 3. 3. 3. 3. 3. 2. 3. 2.]\n",
      " [1. 3. 3. 2. 3. 1. 2. 0. 3. 2.]\n",
      " [3. 3. 3. 3. 3. 2. 2. 3. 2. 2.]\n",
      " [3. 3. 2. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 2. 2. 3. 0. 0. 0. 2. 2.]\n",
      " [1. 3. 3. 3. 3. 1. 2. 0. 2. 1.]]\n",
      "Episode number:  9500\n",
      "------------------------------------------\n",
      "[[2. 2. 3. 1. 3. 0. 3. 3. 3. 2.]\n",
      " [2. 1. 1. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [3. 3. 1. 3. 3. 3. 3. 0. 2. 2.]\n",
      " [3. 2. 2. 3. 3. 0. 3. 3. 3. 2.]\n",
      " [1. 1. 3. 3. 1. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 1. 3. 3. 0. 0. 2. 2. 1.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 1. 0.]]\n",
      "Episode number:  10000\n",
      "------------------------------------------\n",
      "[[2. 1. 1. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 3. 2. 3. 3. 3. 0. 3. 0. 2.]\n",
      " [1. 0. 0. 3. 3. 2. 0. 0. 3. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 2. 2.]\n",
      " [0. 3. 2. 3. 2. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 2. 3. 3. 0. 0. 3. 2. 2.]\n",
      " [2. 2. 2. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  10500\n",
      "------------------------------------------\n",
      "[[0. 1. 0. 3. 3. 3. 2. 3. 3. 2.]\n",
      " [1. 1. 0. 3. 3. 3. 3. 2. 2. 2.]\n",
      " [0. 3. 3. 3. 3. 0. 0. 0. 2. 2.]\n",
      " [3. 2. 3. 1. 0. 2. 3. 3. 2. 2.]\n",
      " [3. 3. 2. 3. 2. 3. 1. 2. 1. 1.]\n",
      " [2. 3. 2. 3. 3. 0. 0. 2. 1. 2.]\n",
      " [1. 2. 2. 2. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  11000\n",
      "------------------------------------------\n",
      "[[3. 3. 1. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 3. 3. 3. 0. 3. 3. 3. 3. 2.]\n",
      " [3. 3. 1. 0. 3. 2. 0. 0. 2. 2.]\n",
      " [3. 2. 3. 3. 2. 3. 2. 3. 1. 2.]\n",
      " [2. 3. 3. 3. 1. 2. 1. 2. 1. 1.]\n",
      " [3. 3. 2. 0. 3. 0. 0. 3. 1. 3.]\n",
      " [3. 3. 1. 3. 3. 1. 2. 0. 1. 0.]]\n",
      "Episode number:  11500\n",
      "------------------------------------------\n",
      "[[3. 1. 0. 1. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 3. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 0. 3. 3. 3. 0. 0. 2. 0. 2.]\n",
      " [3. 3. 2. 3. 3. 2. 0. 3. 3. 2.]\n",
      " [2. 3. 3. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [2. 1. 3. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [1. 1. 3. 3. 3. 1. 2. 0. 2. 0.]]\n",
      "Episode number:  12000\n",
      "------------------------------------------\n",
      "[[2. 0. 0. 2. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 1. 0. 3. 3. 1. 2. 0. 0. 2.]\n",
      " [0. 1. 3. 3. 3. 1. 0. 0. 2. 2.]\n",
      " [3. 2. 3. 3. 3. 3. 0. 3. 2. 2.]\n",
      " [3. 3. 3. 2. 2. 3. 1. 2. 1. 1.]\n",
      " [2. 2. 3. 2. 3. 0. 0. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  12500\n",
      "------------------------------------------\n",
      "[[0. 2. 0. 3. 2. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 2. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [1. 0. 2. 3. 0. 3. 3. 0. 0. 2.]\n",
      " [3. 3. 3. 2. 3. 0. 0. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [1. 0. 3. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [0. 3. 0. 1. 3. 1. 2. 0. 1. 1.]]\n",
      "Episode number:  13000\n",
      "------------------------------------------\n",
      "[[3. 1. 3. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 2. 3. 3. 3. 3. 0. 0. 2.]\n",
      " [0. 3. 1. 3. 3. 3. 3. 0. 2. 2.]\n",
      " [0. 3. 1. 3. 2. 0. 1. 3. 2. 2.]\n",
      " [2. 3. 3. 3. 0. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 3. 2. 0. 0. 2. 2. 3.]\n",
      " [2. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  13500\n",
      "------------------------------------------\n",
      "[[3. 3. 2. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 3. 3. 3. 2. 3. 3. 3. 0. 2.]\n",
      " [3. 3. 2. 3. 2. 0. 0. 0. 2. 2.]\n",
      " [2. 3. 2. 3. 0. 0. 2. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 3. 2. 0. 0. 2. 2. 3.]\n",
      " [3. 2. 3. 1. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  14000\n",
      "------------------------------------------\n",
      "[[0. 3. 3. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 0. 3. 3. 3. 2. 3. 0. 3. 2.]\n",
      " [0. 0. 3. 3. 3. 0. 2. 0. 3. 2.]\n",
      " [0. 3. 3. 2. 3. 2. 3. 3. 2. 2.]\n",
      " [3. 3. 3. 2. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 3. 2. 0. 0. 2. 2. 1.]\n",
      " [0. 3. 3. 2. 3. 1. 2. 0. 2. 2.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  14500\n",
      "------------------------------------------\n",
      "[[0. 3. 3. 0. 2. 2. 3. 3. 3. 2.]\n",
      " [0. 1. 3. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [0. 1. 2. 0. 3. 0. 2. 0. 2. 2.]\n",
      " [2. 3. 0. 0. 3. 3. 2. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [1. 2. 2. 3. 0. 0. 0. 2. 2. 2.]\n",
      " [3. 3. 3. 1. 3. 1. 2. 0. 3. 2.]]\n",
      "Episode number:  15000\n",
      "------------------------------------------\n",
      "[[0. 0. 0. 2. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 3. 2. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [0. 0. 3. 3. 3. 0. 3. 0. 2. 2.]\n",
      " [3. 2. 2. 3. 3. 3. 2. 3. 2. 2.]\n",
      " [3. 3. 1. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [1. 3. 2. 3. 3. 0. 0. 2. 1. 2.]\n",
      " [1. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  15500\n",
      "------------------------------------------\n",
      "[[0. 2. 1. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 2. 2. 0. 0. 3. 0. 3. 0. 2.]\n",
      " [3. 2. 3. 3. 1. 3. 3. 0. 3. 2.]\n",
      " [3. 3. 2. 3. 3. 2. 3. 3. 2. 2.]\n",
      " [2. 2. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [1. 3. 2. 3. 3. 0. 0. 2. 2. 1.]\n",
      " [1. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  16000\n",
      "------------------------------------------\n",
      "[[0. 0. 1. 0. 2. 3. 3. 3. 3. 2.]\n",
      " [0. 1. 3. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [1. 3. 0. 1. 3. 3. 3. 1. 3. 2.]\n",
      " [3. 2. 3. 3. 3. 3. 2. 3. 3. 2.]\n",
      " [3. 2. 1. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [2. 2. 0. 3. 3. 0. 0. 2. 1. 2.]\n",
      " [3. 3. 3. 2. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  16500\n",
      "------------------------------------------\n",
      "[[2. 2. 3. 2. 3. 0. 3. 3. 3. 2.]\n",
      " [0. 2. 2. 3. 3. 3. 0. 0. 0. 2.]\n",
      " [0. 0. 1. 3. 2. 0. 3. 0. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 2. 1. 3. 2. 2.]\n",
      " [3. 0. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [2. 1. 3. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [3. 1. 2. 0. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  17000\n",
      "------------------------------------------\n",
      "[[0. 1. 0. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 3. 3. 1. 3. 3. 3. 0. 2.]\n",
      " [0. 3. 3. 2. 3. 3. 3. 0. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 1. 3. 2. 2.]\n",
      " [3. 3. 3. 3. 0. 0. 1. 2. 1. 1.]\n",
      " [3. 2. 3. 3. 2. 0. 0. 2. 1. 2.]\n",
      " [1. 3. 2. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  17500\n",
      "------------------------------------------\n",
      "[[0. 3. 0. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 3. 3. 0. 3. 3. 0. 3. 0. 2.]\n",
      " [1. 3. 3. 3. 0. 3. 3. 0. 1. 2.]\n",
      " [3. 3. 3. 3. 3. 2. 0. 3. 1. 2.]\n",
      " [2. 2. 3. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 1. 2. 3. 0. 0. 2. 2. 1.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  18000\n",
      "------------------------------------------\n",
      "[[2. 2. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 1. 1. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 1. 3. 2. 0. 3. 0. 1. 0. 2.]\n",
      " [3. 0. 3. 3. 3. 0. 2. 3. 2. 2.]\n",
      " [2. 3. 3. 3. 3. 1. 1. 2. 1. 1.]\n",
      " [1. 2. 2. 3. 3. 0. 0. 2. 2. 1.]\n",
      " [1. 3. 2. 3. 3. 1. 2. 0. 2. 1.]]\n",
      "Episode number:  18500\n",
      "------------------------------------------\n",
      "[[0. 2. 0. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 2. 3. 3. 1. 3. 0. 0. 0. 2.]\n",
      " [1. 3. 1. 3. 0. 3. 3. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 2. 1. 3. 1. 2.]\n",
      " [2. 0. 1. 0. 2. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 0. 0. 2. 1. 2.]\n",
      " [0. 1. 2. 3. 3. 1. 2. 0. 3. 2.]]\n",
      "Episode number:  19000\n",
      "------------------------------------------\n",
      "[[0. 2. 3. 1. 3. 2. 3. 3. 3. 2.]\n",
      " [0. 2. 0. 3. 3. 0. 3. 0. 0. 2.]\n",
      " [3. 0. 3. 0. 3. 1. 3. 0. 0. 2.]\n",
      " [2. 3. 3. 3. 3. 3. 1. 3. 3. 2.]\n",
      " [2. 3. 3. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 2. 3. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [1. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  19500\n",
      "------------------------------------------\n",
      "[[3. 2. 2. 1. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 3. 3. 0. 3. 3. 3. 3. 2. 2.]\n",
      " [0. 3. 3. 3. 0. 1. 3. 0. 3. 2.]\n",
      " [3. 2. 3. 3. 2. 3. 3. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 2. 3. 0. 0. 2. 2. 3.]\n",
      " [2. 2. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  20000\n",
      "------------------------------------------\n",
      "[[0. 3. 2. 0. 1. 3. 3. 3. 3. 2.]\n",
      " [3. 2. 0. 3. 3. 3. 0. 3. 2. 2.]\n",
      " [3. 0. 0. 2. 2. 2. 3. 0. 3. 2.]\n",
      " [0. 2. 3. 3. 1. 3. 0. 3. 2. 2.]\n",
      " [2. 3. 3. 3. 3. 0. 1. 2. 1. 1.]\n",
      " [3. 3. 2. 3. 2. 0. 0. 2. 2. 3.]\n",
      " [3. 2. 2. 1. 3. 1. 2. 0. 3. 1.]]\n",
      "Episode number:  20500\n",
      "------------------------------------------\n",
      "[[3. 0. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 0. 0. 2.]\n",
      " [1. 3. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 0. 2. 3. 2. 2. 1. 3. 3. 2.]\n",
      " [3. 2. 3. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [2. 3. 2. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 1. 1.]]\n",
      "Episode number:  21000\n",
      "------------------------------------------\n",
      "[[0. 2. 3. 1. 3. 3. 3. 2. 3. 2.]\n",
      " [1. 0. 2. 3. 3. 3. 0. 3. 3. 2.]\n",
      " [0. 3. 3. 3. 3. 3. 0. 0. 2. 2.]\n",
      " [3. 2. 3. 3. 1. 3. 1. 3. 2. 2.]\n",
      " [2. 3. 2. 2. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 2. 3. 0. 0. 0. 0. 2. 2. 1.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  21500\n",
      "------------------------------------------\n",
      "[[0. 3. 0. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 0. 2. 3. 3. 3. 3. 0. 3. 2.]\n",
      " [2. 1. 3. 3. 0. 3. 0. 0. 2. 2.]\n",
      " [2. 3. 2. 0. 3. 2. 1. 3. 2. 2.]\n",
      " [3. 3. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [3. 3. 3. 3. 0. 0. 0. 2. 2. 1.]\n",
      " [2. 3. 1. 0. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  22000\n",
      "------------------------------------------\n",
      "[[3. 3. 3. 3. 0. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 3. 3. 3. 3. 3. 0. 3. 2.]\n",
      " [1. 0. 3. 0. 3. 3. 3. 0. 0. 2.]\n",
      " [2. 2. 3. 3. 2. 2. 3. 3. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 3. 0. 3. 0. 0. 2. 2. 3.]\n",
      " [3. 3. 3. 1. 3. 1. 2. 0. 2. 3.]]\n",
      "Episode number:  22500\n",
      "------------------------------------------\n",
      "[[2. 0. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 1. 0. 3. 3. 3. 1. 2. 3. 2.]\n",
      " [0. 1. 3. 0. 3. 0. 1. 0. 0. 2.]\n",
      " [0. 3. 3. 3. 3. 3. 3. 3. 2. 2.]\n",
      " [1. 2. 2. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 0. 3. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 2. 0.]]\n",
      "Episode number:  23000\n",
      "------------------------------------------\n",
      "[[0. 1. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 0. 0. 3. 3. 3. 3. 3. 1. 2.]\n",
      " [0. 3. 3. 3. 3. 3. 0. 0. 2. 2.]\n",
      " [3. 3. 3. 3. 2. 0. 0. 3. 2. 2.]\n",
      " [2. 3. 1. 2. 3. 0. 1. 2. 1. 1.]\n",
      " [2. 2. 0. 3. 1. 0. 0. 2. 2. 2.]\n",
      " [3. 3. 3. 0. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  23500\n",
      "------------------------------------------\n",
      "[[0. 3. 1. 1. 3. 0. 3. 3. 3. 2.]\n",
      " [3. 3. 3. 0. 3. 3. 0. 3. 3. 2.]\n",
      " [0. 0. 2. 3. 0. 3. 3. 0. 0. 2.]\n",
      " [3. 3. 2. 3. 3. 2. 0. 3. 2. 2.]\n",
      " [2. 3. 2. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 3. 3. 2. 0. 0. 2. 2. 2.]\n",
      " [0. 3. 3. 2. 3. 1. 2. 0. 1. 1.]]\n",
      "Episode number:  24000\n",
      "------------------------------------------\n",
      "[[3. 3. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 3. 0. 3. 3. 3. 3. 0. 3. 2.]\n",
      " [0. 0. 3. 0. 3. 3. 3. 0. 2. 2.]\n",
      " [3. 2. 0. 0. 3. 3. 1. 3. 2. 2.]\n",
      " [3. 2. 0. 2. 3. 3. 1. 2. 1. 1.]\n",
      " [2. 3. 3. 3. 2. 0. 0. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 1. 1.]]\n",
      "Episode number:  24500\n",
      "------------------------------------------\n",
      "[[0. 0. 0. 3. 2. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 2. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [3. 3. 0. 3. 3. 2. 3. 0. 2. 2.]\n",
      " [2. 2. 2. 3. 2. 2. 3. 3. 2. 2.]\n",
      " [2. 3. 3. 3. 2. 3. 1. 2. 1. 1.]\n",
      " [1. 3. 1. 2. 3. 0. 0. 2. 2. 3.]\n",
      " [2. 3. 3. 3. 3. 1. 2. 0. 2. 2.]]\n",
      "Episode number:  25000\n",
      "------------------------------------------\n",
      "[[1. 3. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [2. 3. 2. 3. 3. 3. 3. 3. 0. 2.]\n",
      " [2. 3. 3. 3. 0. 0. 1. 0. 0. 2.]\n",
      " [3. 0. 3. 3. 3. 2. 3. 3. 2. 2.]\n",
      " [2. 2. 3. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [3. 1. 2. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [0. 3. 3. 3. 3. 1. 2. 0. 1. 2.]]\n",
      "Episode number:  25500\n",
      "------------------------------------------\n",
      "[[0. 1. 0. 1. 0. 3. 3. 3. 3. 2.]\n",
      " [3. 2. 3. 3. 1. 3. 3. 0. 0. 2.]\n",
      " [0. 3. 3. 3. 3. 3. 2. 0. 2. 2.]\n",
      " [3. 0. 3. 3. 3. 2. 2. 3. 2. 2.]\n",
      " [1. 3. 3. 2. 3. 2. 1. 2. 1. 1.]\n",
      " [1. 3. 3. 1. 0. 0. 0. 2. 2. 2.]\n",
      " [3. 2. 3. 3. 3. 1. 2. 0. 1. 1.]]\n",
      "Episode number:  26000\n",
      "------------------------------------------\n",
      "[[3. 3. 3. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 0. 3. 0. 3. 3. 0. 0. 0. 2.]\n",
      " [3. 2. 3. 3. 3. 3. 2. 0. 2. 2.]\n",
      " [3. 3. 2. 3. 2. 1. 0. 3. 2. 2.]\n",
      " [2. 2. 3. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [3. 1. 3. 3. 3. 0. 0. 2. 2. 1.]\n",
      " [0. 0. 3. 3. 3. 1. 2. 0. 1. 0.]]\n",
      "Episode number:  26500\n",
      "------------------------------------------\n",
      "[[0. 3. 3. 3. 3. 1. 3. 3. 3. 2.]\n",
      " [3. 0. 0. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 0. 0. 2. 3. 1. 3. 0. 0. 2.]\n",
      " [3. 3. 2. 3. 3. 3. 0. 3. 2. 2.]\n",
      " [3. 2. 3. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 3. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [3. 1. 2. 3. 3. 1. 2. 0. 2. 1.]]\n",
      "Episode number:  27000\n",
      "------------------------------------------\n",
      "[[0. 0. 0. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 3. 3. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 3. 3. 0. 3. 0. 3. 0. 0. 2.]\n",
      " [3. 3. 3. 1. 1. 3. 3. 3. 2. 2.]\n",
      " [1. 3. 2. 3. 3. 3. 1. 2. 1. 1.]\n",
      " [0. 2. 3. 3. 0. 0. 0. 2. 2. 1.]\n",
      " [1. 3. 3. 3. 3. 1. 2. 0. 1. 1.]]\n",
      "Episode number:  27500\n",
      "------------------------------------------\n",
      "[[0. 3. 1. 1. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 3. 0. 0. 3. 3. 1. 0. 0. 2.]\n",
      " [0. 3. 2. 2. 0. 2. 2. 0. 3. 2.]\n",
      " [0. 0. 3. 0. 3. 3. 2. 3. 1. 2.]\n",
      " [2. 2. 2. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 2. 3. 3. 3. 0. 0. 2. 2. 1.]\n",
      " [3. 0. 2. 3. 3. 1. 2. 0. 3. 2.]]\n",
      "Episode number:  28000\n",
      "------------------------------------------\n",
      "[[2. 1. 1. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [3. 0. 3. 3. 0. 3. 3. 3. 3. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 1. 3. 3. 2.]\n",
      " [3. 3. 2. 3. 3. 3. 2. 3. 2. 2.]\n",
      " [3. 2. 2. 3. 3. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 1. 3. 3. 0. 0. 2. 2. 1.]\n",
      " [3. 3. 3. 3. 3. 1. 2. 0. 3. 2.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:  28500\n",
      "------------------------------------------\n",
      "[[2. 3. 0. 0. 0. 3. 3. 3. 3. 2.]\n",
      " [3. 0. 1. 0. 3. 0. 0. 3. 3. 2.]\n",
      " [0. 1. 3. 1. 3. 3. 0. 0. 3. 2.]\n",
      " [2. 3. 0. 3. 3. 0. 3. 3. 2. 2.]\n",
      " [2. 1. 3. 3. 2. 3. 1. 2. 1. 1.]\n",
      " [2. 2. 2. 3. 3. 0. 0. 2. 2. 2.]\n",
      " [1. 3. 1. 3. 3. 1. 2. 0. 3. 1.]]\n",
      "Episode number:  29000\n",
      "------------------------------------------\n",
      "[[0. 2. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 1. 3. 1. 3. 3. 3. 3. 0. 2.]\n",
      " [1. 1. 3. 3. 3. 3. 3. 2. 1. 2.]\n",
      " [0. 0. 3. 3. 3. 2. 3. 3. 2. 2.]\n",
      " [3. 3. 2. 3. 3. 0. 1. 2. 1. 1.]\n",
      " [0. 3. 2. 3. 2. 0. 0. 2. 1. 2.]\n",
      " [3. 2. 3. 3. 3. 1. 2. 0. 1. 1.]]\n",
      "Episode number:  29500\n",
      "------------------------------------------\n",
      "[[2. 3. 0. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 3. 3. 1. 3. 3. 3. 3. 2.]\n",
      " [0. 3. 3. 0. 3. 3. 3. 0. 2. 2.]\n",
      " [3. 2. 3. 2. 0. 3. 1. 3. 2. 2.]\n",
      " [2. 2. 0. 2. 3. 3. 1. 2. 1. 1.]\n",
      " [2. 3. 2. 3. 3. 0. 0. 2. 2. 3.]\n",
      " [3. 2. 3. 3. 3. 1. 2. 0. 3. 2.]]\n",
      "Episode number:  30000\n",
      "------------------------------------------\n",
      "[[3. 2. 3. 0. 3. 3. 3. 3. 3. 2.]\n",
      " [1. 0. 3. 1. 3. 3. 0. 2. 3. 2.]\n",
      " [2. 3. 2. 2. 3. 3. 0. 2. 3. 2.]\n",
      " [3. 2. 3. 3. 0. 3. 2. 3. 2. 2.]\n",
      " [2. 3. 2. 2. 2. 2. 1. 2. 1. 1.]\n",
      " [2. 3. 0. 3. 2. 0. 0. 2. 1. 2.]\n",
      " [1. 3. 1. 3. 3. 1. 2. 0. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "env_info = {\"grid_height\": 7, \"grid_width\": 10, \"seed\": 0}\n",
    "agent_info = {\"discount\": 1, \"step_size\": 0.01, \"seed\": 0}\n",
    "\n",
    "true_values_file = \"optimal_policy_q_value_fn.npy\"\n",
    "_ = run_experiment(env_info, agent_info, num_episodes=30000, experiment_name=\"SARSA in Windy Grid World\",\n",
    "                   plot_freq=500, true_values_file=true_values_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-omega",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-jefferson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-findings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-circulation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
